{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2714539/3555461397.py:26: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  initialize(config_path=\"../configs\", job_name=\"job\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hydra.initialize()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import rootutils\n",
    "\n",
    "rootutils.setup_root(\"./\", indicator=\".project-root\", pythonpath=True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import hydra\n",
    "import torch\n",
    "import torch_geometric\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from topobenchmarkx.data.preprocessor import PreProcessor\n",
    "from topobenchmarkx.dataloader.dataloader import TBXDataloader\n",
    "from topobenchmarkx.data.loaders import GraphLoader\n",
    "\n",
    "from topobenchmarkx.utils.config_resolvers import (\n",
    "    get_default_transform,\n",
    "    get_monitor_metric,\n",
    "    get_monitor_mode,\n",
    "    infer_in_channels,\n",
    ")\n",
    "\n",
    "\n",
    "initialize(config_path=\"../configs\", job_name=\"job\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = compose(config_name=\"run.yaml\", return_hydra_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_loader = GraphLoader(cfg.dataset.loader.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting /home/lev/projects/nlp/TopoBenchmarkX/datasets/graph/nyu/LanguageDataset/raw/LanguageDataset.zip\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset, dataset_dir = graph_loader.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataTransform.__init__() missing 1 required positional argument: 'transform_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m \u001b[43mPreProcessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtransforms\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/nlp/TopoBenchmarkX/topobenchmarkx/data/preprocessor/preprocessor.py:46\u001b[0m, in \u001b[0;36mPreProcessor.__init__\u001b[0;34m(self, dataset, data_dir, transforms_config, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transforms_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms_applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     pre_transform \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstantiate_pre_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransforms_config\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_data_dir, \u001b[38;5;28;01mNone\u001b[39;00m, pre_transform, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_transform_parameters()\n",
      "File \u001b[0;32m~/projects/nlp/TopoBenchmarkX/topobenchmarkx/data/preprocessor/preprocessor.py:108\u001b[0m, in \u001b[0;36mPreProcessor.instantiate_pre_transform\u001b[0;34m(self, data_dir, transforms_config)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Instantiate the pre-transforms.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    Pre-transform object.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    107\u001b[0m pre_transforms_dict \u001b[38;5;241m=\u001b[39m hydra\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39minstantiate(transforms_config)\n\u001b[0;32m--> 108\u001b[0m pre_transforms_dict \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mDataTransform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransforms_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    112\u001b[0m pre_transforms \u001b[38;5;241m=\u001b[39m torch_geometric\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mCompose(\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mlist\u001b[39m(pre_transforms_dict\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    114\u001b[0m )\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_processed_data_dir(\n\u001b[1;32m    116\u001b[0m     pre_transforms_dict, data_dir, transforms_config\n\u001b[1;32m    117\u001b[0m )\n",
      "File \u001b[0;32m~/projects/nlp/TopoBenchmarkX/topobenchmarkx/data/preprocessor/preprocessor.py:109\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Instantiate the pre-transforms.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    Pre-transform object.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    107\u001b[0m pre_transforms_dict \u001b[38;5;241m=\u001b[39m hydra\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39minstantiate(transforms_config)\n\u001b[1;32m    108\u001b[0m pre_transforms_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 109\u001b[0m     key: \u001b[43mDataTransform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m transforms_config\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    111\u001b[0m }\n\u001b[1;32m    112\u001b[0m pre_transforms \u001b[38;5;241m=\u001b[39m torch_geometric\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mCompose(\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mlist\u001b[39m(pre_transforms_dict\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    114\u001b[0m )\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_processed_data_dir(\n\u001b[1;32m    116\u001b[0m     pre_transforms_dict, data_dir, transforms_config\n\u001b[1;32m    117\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: DataTransform.__init__() missing 1 required positional argument: 'transform_name'"
     ]
    }
   ],
   "source": [
    "preprocessor = PreProcessor(dataset, dataset_dir, cfg['transforms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_manipulations': {'_target_': 'topobenchmarkx.transforms.data_transform.DataTransform', 'transform_name': 'Attention2Graph', 'transform_type': 'data manipulation', 'threshold': 0.1}, 'liftings': {'graph2hypergraph_lifting': {'_target_': 'topobenchmarkx.transforms.data_transform.DataTransform', 'transform_type': 'lifting', 'transform_name': 'HypergraphKHopLifting', 'k_value': 1}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lev/miniconda3/envs/tbx_nlp/lib/python3.11/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2495008"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.data.attention_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 400], attention_scores=[400], attention_shape=[2], ids=[20], tokens=[20], tags=[20])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_index = 0\n",
    "# attention_scores = []\n",
    "# for token_list in dataset.data.tokens:\n",
    "#     end_index = start_index + len(token_list) * len(token_list)\n",
    "#     attention_scores_sentence = dataset.data.attention_scores[start_index:end_index]\n",
    "#     start_index = end_index\n",
    "#     attention_scores_sentence = torch.reshape(attention_scores_sentence, (len(token_list), len(token_list)))\n",
    "#     attention_scores.append(attention_scores_sentence)\n",
    "# tokens = dataset.data.tokens\n",
    "# ids = dataset.data.ids\n",
    "# tags = dataset.data.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 32\n",
    "num_sentences = len(ids)/num_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence we now have the following data.\n",
    "\n",
    "The length of each attribute is num_heads*num_sentences:\n",
    "1. tokens - each element is a list of length sentence_length\n",
    "2. ids - each element is a list of length sentence_length\n",
    "3. tags - each element is a list of length sentence_length\n",
    "4. attention_scores - each element is a tensor of shape (sentence_length, sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of tuple occurences\n",
    "graph_2s = []\n",
    "\n",
    "# the attention score threshold to consider relation between two tokens \n",
    "threshold = 0.0001\n",
    "\n",
    "# list of tokens to avoid in relations\n",
    "tokens_avoid = ['<|begin_of_text|>'.lower(), ''.lower()]\n",
    "\n",
    "# iterate over all sentences for all attention heads\n",
    "for sentence in range(len(tokens)):\n",
    "    \n",
    "    current_attention = attention_scores[sentence]\n",
    "    current_tokens = tokens[sentence]\n",
    "    \n",
    "    for row in range(len(current_attention)):\n",
    "        \n",
    "        for col in range(0,row+1):\n",
    "            \n",
    "            if row != col and current_attention[row][col] >= threshold:\n",
    "                word1 = current_tokens[row].lower().strip()\n",
    "                word2 = current_tokens[col].lower().strip()\n",
    "                \n",
    "                # Skip tokens that are empty or beginning of text indicators\n",
    "                if word1 in tokens_avoid or word2 in tokens_avoid or word1.isnumeric() or word2.isnumeric():\n",
    "                    continue\n",
    "                relation = ()\n",
    "                \n",
    "                # Create an ordered tuple for consistency\n",
    "                if word1 < word2:\n",
    "                    relation = (word1,word2)\n",
    "                else:\n",
    "                    relation = (word2,word1)\n",
    "\n",
    "                graph_2s.append(relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001569\n"
     ]
    }
   ],
   "source": [
    "print(len(graph_2s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "counter=collections.Counter(graph_2s)\n",
    "# print(collections.OrderedDict(sorted(counter.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [4.6254e-01, 5.3746e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [4.2101e-01, 9.7475e-02, 4.8151e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [3.7420e-01, 3.1179e-02, 1.3714e-01, 4.5748e-01, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [7.7811e-01, 6.8530e-04, 2.6457e-03, 9.0795e-02, 1.2776e-01, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [4.3133e-01, 1.2016e-02, 1.0733e-02, 2.8916e-02, 5.1130e-02, 4.6587e-01,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [4.2843e-01, 6.6626e-03, 1.3055e-02, 4.7056e-03, 8.4519e-03, 5.4083e-02,\n",
       "         4.8461e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [3.4764e-01, 4.0452e-03, 7.2805e-03, 2.3914e-03, 2.3553e-03, 1.1718e-02,\n",
       "         8.1760e-02, 5.4281e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [3.1476e-01, 2.0011e-03, 2.3785e-03, 1.5471e-03, 1.1528e-03, 3.4818e-03,\n",
       "         1.0619e-02, 7.0552e-02, 5.9350e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [2.4278e-01, 7.1517e-04, 1.0854e-03, 1.0017e-03, 9.2751e-04, 3.2553e-03,\n",
       "         8.7617e-03, 3.3378e-02, 1.6144e-01, 5.4666e-01, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [1.9087e-01, 6.9281e-04, 4.4001e-04, 4.2536e-04, 6.8224e-04, 2.7703e-03,\n",
       "         6.2806e-03, 8.4009e-03, 2.3959e-02, 1.0328e-01, 6.6220e-01, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [2.6753e-01, 1.2780e-03, 1.1696e-03, 5.4049e-04, 6.3220e-04, 1.7825e-03,\n",
       "         1.7669e-02, 2.9694e-02, 2.0154e-02, 9.8352e-02, 2.4472e-02, 5.3673e-01,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [3.6325e-01, 5.5352e-03, 2.4752e-03, 1.3191e-03, 7.2738e-04, 1.9392e-03,\n",
       "         2.6853e-02, 2.4892e-02, 4.7821e-03, 1.0479e-02, 4.9252e-03, 9.6939e-02,\n",
       "         4.5588e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [2.6703e-01, 1.6259e-03, 1.4267e-03, 1.3090e-03, 3.7643e-04, 2.8772e-03,\n",
       "         9.6214e-03, 1.2213e-02, 6.2202e-03, 1.1960e-02, 4.8540e-03, 4.2025e-02,\n",
       "         1.3978e-01, 4.9869e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [3.0759e-01, 7.9129e-03, 2.2542e-03, 3.4622e-03, 1.7779e-03, 3.1254e-03,\n",
       "         5.0459e-03, 6.2305e-03, 2.1069e-03, 1.4180e-02, 5.0137e-03, 5.9892e-02,\n",
       "         4.2519e-02, 7.6097e-02, 4.6279e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [3.3704e-01, 1.2920e-03, 4.2633e-04, 6.5452e-04, 9.0092e-04, 1.3439e-03,\n",
       "         1.6715e-03, 2.3698e-03, 8.0440e-04, 2.3760e-03, 1.6231e-03, 6.2169e-03,\n",
       "         1.3391e-02, 1.8584e-02, 1.2125e-01, 4.9005e-01, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [2.6125e-01, 1.1021e-03, 2.1557e-04, 2.3425e-04, 2.5149e-04, 3.0074e-03,\n",
       "         2.6816e-03, 2.7123e-03, 6.8079e-04, 2.1213e-03, 2.8233e-04, 1.4818e-02,\n",
       "         7.3490e-03, 7.4933e-03, 8.6302e-02, 6.1475e-02, 5.4802e-01, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [1.9283e-01, 1.3986e-03, 4.8067e-04, 3.7613e-04, 2.4888e-04, 4.0859e-03,\n",
       "         2.7159e-03, 4.0854e-03, 1.9484e-03, 3.5240e-03, 1.4965e-03, 1.2528e-02,\n",
       "         9.0609e-03, 8.3385e-03, 1.8343e-02, 3.0147e-02, 1.0601e-01, 6.0238e-01,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [2.3280e-01, 1.0317e-03, 4.3144e-04, 2.1228e-04, 2.3688e-04, 1.1032e-03,\n",
       "         1.7312e-03, 1.9840e-03, 1.1670e-03, 9.0450e-04, 1.2566e-03, 2.3647e-03,\n",
       "         2.1894e-03, 5.1913e-03, 6.5635e-03, 9.9111e-03, 1.7750e-02, 7.8872e-02,\n",
       "         6.3430e-01, 0.0000e+00],\n",
       "        [2.6816e-01, 7.8938e-04, 5.0931e-04, 3.1422e-04, 1.7340e-04, 1.5555e-03,\n",
       "         1.0228e-03, 2.1595e-03, 2.4593e-03, 7.7517e-03, 6.0901e-04, 2.3456e-02,\n",
       "         5.5981e-03, 8.5362e-03, 6.3944e-02, 2.5942e-02, 9.3970e-02, 2.0980e-02,\n",
       "         4.7699e-02, 4.2437e-01]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of tuple occurences\n",
    "attention_head = 0\n",
    "graph_2s_head = []\n",
    "\n",
    "# the attention score threshold to consider relation between two tokens \n",
    "threshold = 0.0001\n",
    "\n",
    "# list of tokens to avoid in relations\n",
    "tokens_avoid = ['<|begin_of_text|>'.lower(), ''.lower()]\n",
    "\n",
    "# iterate over all sentences for the first attention head\n",
    "for sentence in range(attention_head,len(tokens), num_heads):\n",
    "    \n",
    "    current_attention = attention_scores[sentence]\n",
    "    current_tokens = tokens[sentence]\n",
    "    \n",
    "    for row in range(len(current_attention)):\n",
    "        \n",
    "        for col in range(0,row+1):\n",
    "            \n",
    "            if row != col and current_attention[row][col] >= threshold:\n",
    "                word1 = current_tokens[row].lower().strip()\n",
    "                word2 = current_tokens[col].lower().strip()\n",
    "                \n",
    "                # Skip tokens that are empty or beginning of text indicators\n",
    "                if word1 in tokens_avoid or word2 in tokens_avoid or word1.isnumeric() or word2.isnumeric():\n",
    "                    continue\n",
    "                relation = ()\n",
    "                \n",
    "                # Create an ordered tuple for consistency\n",
    "                if word1 < word2:\n",
    "                    relation = (word1,word2)\n",
    "                else:\n",
    "                    relation = (word2,word1)\n",
    "\n",
    "                graph_2s_head.append(relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32672\n"
     ]
    }
   ],
   "source": [
    "print(len(graph_2s_head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "counter2=collections.Counter(graph_2s_head)\n",
    "# print(collections.OrderedDict(sorted(counter2.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tbx_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
